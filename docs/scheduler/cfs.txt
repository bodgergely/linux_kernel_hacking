Completely Fair Scheduler
=========================

Documentation/scheduler/sched-design-CFS.rst

Links:
------

https://blog.shichao.io/2015/07/22/relationships_among_nice_priority_and_weight_in_linux_kernel.html


Man page
--------

man 7 sched
man 7 cpuset
man 2 setpriority

In general see:

man sched_


Scheduler debugging, tuning
---------------------------

/proc/sched_debug has lots of internal info.

The below files can be set/get with sysctl

/proc/sys/kernel/sched_autogroup_enabled
/proc/sys/kernel/sched_cfs_bandwidth_slice_us
/proc/sys/kernel/sched_child_runs_first
/proc/sys/kernel/sched_domain
/proc/sys/kernel/sched_latency_ns
/proc/sys/kernel/sched_migration_cost_ns
/proc/sys/kernel/sched_min_granularity_ns
/proc/sys/kernel/sched_nr_migrate
/proc/sys/kernel/sched_rr_timeslice_ms
/proc/sys/kernel/sched_rt_period_us
/proc/sys/kernel/sched_rt_runtime_us
/proc/sys/kernel/sched_schedstats
/proc/sys/kernel/sched_time_avg_ms
/proc/sys/kernel/sched_tunable_scaling
/proc/sys/kernel/sched_wakeup_granularity_ns

Read them:
for f in `find /proc/sys/kernel -name "sched_*"`; do echo $f; cat $f; done



TLDR
----

Run the task with the smallest p->se.vruntime value.


Virtual Runtime concept
-----------------------

On real hardware, we can run only a single task at once, so we have to
introduce the concept of "virtual runtime."  The virtual runtime of a task
specifies when its next timeslice would start execution on the ideal
multi-tasking CPU described above.  In practice, the virtual runtime of a task
is its actual runtime normalized to the total number of running tasks.


Task picking logic
------------------

**************************************************************************************
*                                                                                    *
* CFS's task picking logic is based on this **p->se.vruntime** value and it is thus  *
* very simple: it always tries to run the task with the smallest p->se.vruntime      *
* value (i.e., the task which executed least so far).  CFS always tries to split     *
* up CPU time between runnable tasks as close to "ideal multitasking hardware" as    *
* possible.                                                                          *
*                                                                                    *
**************************************************************************************


Summing up, CFS works like this: it runs a task a bit, and when the task
schedules (or a scheduler tick happens) the task's CPU usage is "accounted
for": the (small) time it just spent using the physical CPU is added to
p->se.vruntime.  Once p->se.vruntime gets high enough so that another task
becomes the "leftmost task" of the time-ordered rbtree it maintains (plus a
small amount of "granularity" distance relative to the leftmost task so that we
do not over-schedule tasks and trash the cache), then the new leftmost task is
picked and the current task is preempted.



CFS uses nanosecond granularity accounting and does not rely on any jiffies or
other HZ detail.  Thus the CFS scheduler has no notion of "timeslices" in the
way the previous scheduler had, and has no heuristics whatsoever.  There is
only one central tunable (you have to switch on CONFIG_SCHED_DEBUG):

   /proc/sys/kernel/sched_min_granularity_ns

which can be used to tune the scheduler from "desktop" (i.e., low latencies) to
"server" (i.e., good batching) workloads.  It defaults to a setting suitable
for desktop workloads.  SCHED_BATCH is handled by the CFS scheduler module too.


Scheduler code
==============

kernel/sched/core.c
kernel/sched/fair.c - this is the CFS specific scheduler class code

NICE_TO_PRIO - macro to convert nice to prio
PRIO_TO_NICE

Relation: prio = nice + 120



Time accounting
---------------
Nanosecs are used for time accounting.

fair.c has the update_curr() method that is called to update cpu time for the curr task.

void update_process_times(int usertick) is timer.c is called by the timer interrupt!!! (kernel/time/tick-sched.c:tick_sched_handle)
    - calls run_posix_cpu_timers() in posix-cpu-timers.c
        - this file has the posix_cpu_timers_ functions
        - calls the thread_group_cputime() in kernel/sched/cputime.c !!!!
            - calls task_sched_runtime() in sched/core.c
                - calls the registered scheduler class update_curr() to update the running time


sched_prio_to_weight[40] contains the priority-to-weight mappings in sched/core.c!

const int sched_prio_to_weight[40] = {
 /* -20 */     88761,     71755,     56483,     46273,     36291,
 /* -15 */     29154,     23254,     18705,     14949,     11916,
 /* -10 */      9548,      7620,      6100,      4904,      3906,
 /*  -5 */      3121,      2501,      1991,      1586,      1277,
 /*   0 */      1024,       820,       655,       526,       423,
 /*   5 */       335,       272,       215,       172,       137,
 /*  10 */       110,        87,        70,        56,        45,
 /*  15 */        36,        29,        23,        18,        15,
};

The weight is roughly equivalent to 1024/(1.25)^(nice).
There is a per-CPU scheduler. Loadbalancing is required when there are multiple CPUs. Each CPU has its runqueue.

Timer ticks
-----------
Docs/timers/no_hz.rst - IMPORTANT!!!
CONFIG_NO_HZ_COMMON - most likely set to yes - see the doc 

    1. Never omit timer interrupts
    2. Omit timer interrupts when idle the CPU (this is the default setting nowadays - CONFIG_NO_HZ_COMMON=y)
    3. Omit the timer when only one task is runnable - adaptive-ticks!!

nohz=off boot parameter can turn off the second option.
"nohz_full=1,6-8" says that CPUs 1, 6, 7, and 8 are to be adaptive-ticks


timer.c
tick-sched.c
hrtimer.c



Load Balancing
==============

Documentation/cgroup-v1/cpusets.txt
man 7 cpuset

fair.c
cpuset.c

idle_balance()
nohz_newidle_balance()
load_balance()

Patches
-------
https://lkml.org/lkml/2019/9/19/92
https://lkml.org/lkml/2018/12/6/1253

Links
-----
https://hal.archives-ouvertes.fr/hal-01295194/document












